# 1C Codemetadata MCP Server — configuration
# Copy to .env and fill in your values

# =============================================================================
# PATH TO YOUR 1C SOURCE CODE (required)
# =============================================================================
# Export from 1C Configurator: Configuration → Dump config to files
# Point SOURCE_PATH to the directory containing cf/ subdirectory
SOURCE_PATH=./source

# ChromaDB vector index storage (created automatically)
CHROMA_PATH=./chroma_db

# =============================================================================
# EMBEDDING PROVIDERS
# =============================================================================
# Three separate providers let you optimize cost vs speed:
#
#   INDEXING_PROVIDER  — used ONCE for initial ChromaDB bulk fill on first start
#   SEARCH_PROVIDER    — used for EVERY search query (runtime cost)
#   REINDEX_PROVIDER   — used when you manually call reindex after code changes
#
# Simple setup (OpenRouter for everything, works out of the box):
INDEXING_PROVIDER=openrouter
SEARCH_PROVIDER=openrouter
REINDEX_PROVIDER=openrouter
#
# Hybrid setup (recommended if you have Ollama running locally):
# INDEXING_PROVIDER=openrouter   ← cloud for fast initial bulk indexing
# SEARCH_PROVIDER=ollama         ← free local inference for every query
# REINDEX_PROVIDER=ollama        ← free local inference for reindex

# =============================================================================
# API KEYS
# =============================================================================
# OpenRouter (https://openrouter.ai/keys)
# Recommended model: qwen/qwen3-embedding-8b — best for Russian/BSL code
OPENROUTER_API_KEY=sk-or-v1-your-key-here

# OpenAI (https://platform.openai.com/api-keys) — alternative provider
# OPENAI_API_KEY=sk-your-openai-key-here

# =============================================================================
# OLLAMA (optional, for hybrid setup)
# =============================================================================
# If Ollama is running on your host machine:
# OLLAMA_BASE_URL=http://host.docker.internal:11434
# OLLAMA_MODEL=qwen3-embedding:8b

# =============================================================================
# INDEXING SETTINGS
# =============================================================================
# Auto-index SQLite on startup (fast, always recommended)
AUTO_INDEX=true

# Auto-index ChromaDB on first start (cloud calls — set false after first run)
CHROMADB_AUTO_INDEX=true

CHUNK_SIZE=1000
CHUNK_OVERLAP=100

# Parallel embedding requests (OpenRouter supports high concurrency)
# 5 = safe default; 10 = faster but may hit rate limits
EMBEDDING_CONCURRENCY=5
EMBEDDING_BATCH_SIZE=10

# =============================================================================
# SERVER
# =============================================================================
MCP_PORT=8000
